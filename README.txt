#############################################################################################
##################                 CSE538 Assignment 1                        ###############
#############################################################################################


Student Name: Salman Qavi
Student ID: 109470856


System:
  Model Name:		MacBook Pro
  Processor Name:	Intel Core i5
  Processor Speed:	2.4 GHz
  Number of Processors:	1
  Number of Cores:	4
  Hyper-Threading:	Enabled
  Memory:		8 GB
  Operation System:	MacOS Mojave 10.146

Software:
  IDE: 			PyCharm 2019.2
  Python Interpreter:	3.7
  Tensorflow: 		1.14.1
  SciPy:		1.3.1

#######################################################################
####### Configuration That Produced the Best Cross-Entropy ############
#######################################################################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 0.001

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    321
Number of Most Illustrative Guessed Incorrectly:  593
Accuracy of Most Illustrative Guesses:             35.1%
Overall Accuracy:                                  34.5%




################################################################
######## Configuration That Produced the Best NCE ##############
################################################################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 2       	# How many words to consider left and right.
  num_skips = 4         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   295
Number of Least Illustrative Guessed Incorrectly: 619
Accuracy of Least Illustrative Guesses:            32.3%
Number of Most Illustrative Guessed Correctly:    333
Number of Most Illustrative Guessed Incorrectly:  581
Accuracy of Most Illustrative Guesses:             36.4%
Overall Accuracy:                                  34.4%


#############################################################################################
##################                 Implementation                             ###############
#############################################################################################

Task 1. Batch Generation

The function 'generate_batch()' generates mini-batches that we use during our training. Each of these batches consist of
input words and a random word selected from its context. The number of words that we draw at random from the contexts is
predefined by the argument 'num_skips'. If we select a sequence of words that is 5 words long ("the students studies a lot"),
then the skip_window is equal to two and there are two skip_window(s) around the center word 'studies'.

In the first 5 lines in the function, the batch and label outputs are defined as variables of size batch_size. Then the span size
is defined (i.e. [skip_window target skip_window] which would be 5 words if skip_windows == 2). Next, inside the first 'while' loop,
a buffer queue is created that can hold a maximum of elements in one span (2 x skip window + 1). As soon as a new word is enqueued
to the buffer, the left most element will be popped from the buffer.

Inside the second while loop, first we select a target word for the span and then randomly select context words using randint() function
within the range of the indices of span. We do not need the target word itself so to avoid it we store it temporarily and skip the loop
if the input word happens to be the target word itself. Finally, we loop to match the input word with a randomly selected word from the
context and store each of them in the buffer queue. When the span reaches the end of the line, the matched batch and the labels are returned.


Task 2. Loss Functions:

a. Cross-Entropy

Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1 (binary).
A high cross-entropy loss value means that the predicted value and the actual value have diverged from each other (not close).
On the other hand, when the predicted value is the same as the actual value then cross-entropy loss is 0 (i.e. there is no loss).

def cross_entropy_loss(inputs, true_w):
    mat = tf.matmul(inputs, true_w, transpose_b=True)
    A = tf.diag_part(mat)  				# A = log(exp({true_w}^T inputs))
    B = tf.log(tf.reduce_sum(tf.exp(mat), axis=1))  	# B = log(\sum{exp({true_w}^T inputs)})
    return tf.subtract(B, A)

In the definition of cross-entropy loss function above, first we multiply one matrix with the transpose of the other matrix and store the
resulting matrix in 'mat' variable. The reason we do this is because we will need it several times in the following statements. The
parameters input correspond to the target word and true_w correspond to the context words around the target word. We only need the diagonal
values of our previously multiplied matrix 'mat' so we use tf.diag_part to extract the elements and store it in A. As log and exponent cancels
out each other, this A becomes our first term. The second term is the sum over all words in the vocabulary. As the vocabulary itself is the
batch size, we multiply these and get the log value to get the sum across all columns in the matrix.


b. NCE Loss


FORMULA USED in accordance to the variable used in my function (please see below or loss_func.py)

    #term1 = Sigmoid(t1.T*inputs + bias_labels - log(sample_size* pr_pd))
    #term2 = Sigmoid(inputs*t2.T + bias_sample - log(sample_size*pr_pn)
    #A = log(term)
    #B = sum(log(1 - term2))
    #nce_loss_result = - (A+B)

We calculate the term A and B separate and then negative them in the end.
In detail the steps for calculating NCE loss are:
1. To get A:
	a. extracting the sample, batch and embedding sizes.
	b. arrays of small corrections are added to tensors
	c. extract the target word embeddings from the weights.
    	d. extract unigram probabilities for the target words.
    	e. extract the target bias for the target words.
	f. calculate the dot product of context and target word embeddings
	g. convert negative words to tensor
	h. calculate the log(sample_size* pr_pd))
	i. subtracting the unigram probabiities from the score and calculating the sigmoid
2. To get B:
	j. extract the negative word embeddings from the given weights.
        k. extract unigram probabilities for the negative words.
	l. extract the negative word bias
	m. calculate the dot product for the context word and negative word embeddings
	n. add negative word bias
	o. calculate the log(sample_size*pr_pn)
	p. calculate the sigmoid value of all the terms
	q. sum over the log value of 1 minus the value obtained in step p.

3. To get NCE Loss Score:
	p. add A and B and subtract the sum from 1


#############################################################################################
def nce_loss(inputs, weights, biases, labels, sample, unigram_prob):
    sample_size = len(sample)
    input_size = inputs.get_shape().as_list()
    batch_size, embedding_size = input_size[0], input_size[1]

    unigram_prob_tensor = tf.convert_to_tensor(unigram_prob, dtype=tf.float32)

    pr_pd = tf.reshape(tf.nn.embedding_lookup(unigram_prob_tensor, labels), [batch_size])
    pr_pn = np.ndarray(shape=(sample_size), dtype=np.float32)

    counter = 0
    while counter < sample_size:
        uni_prob = unigram_prob[sample[counter]]
        pr_pn[counter] = uni_prob
        counter += 1

    t1 = tf.reshape(tf.nn.embedding_lookup(weights, labels), [-1, embedding_size])
    t2 = tf.nn.embedding_lookup(weights, sample)

    tensor1 = tf.diag_part(tf.matmul(inputs, tf.transpose(t1)))
    tensor2 = tf.matmul(inputs, tf.transpose(t2))

    bias_labels = tf.reshape(tf.nn.embedding_lookup(biases, labels), [batch_size])
    bias_sample = tf.nn.embedding_lookup(biases, sample)

    term1 = tf.log(tf.add(sample_size * pr_pd, 1e-10))
    term1 = tf.subtract(tensor1 + bias_labels, term1)
    term1 = tf.sigmoid(term1)

    term2 = tf.log(tf.add(sample_size * pr_pn, 1e-10))
    term2 = tf.subtract(tensor2 + bias_sample, term2)
    term2 = tf.sigmoid(term2)

    A = tf.log(term1 + 1e-10)
    B = tf.reduce_sum(tf.log(1.0 - term2 + 1e-10),1)

    nce_loss_result = tf.negative(tf.add(A, B))
    return nce_loss_result
#############################################################################################



Task 3. Word Analogy

1. read and extract each line from the input file
2. for each line:
	a. split the line using the delimiter ('||') and store the right side of the line after the delimiter
	b. further split the right side of the line into individual words using the delimiter (',') and then strip off any while space
	c. each word is again split using the delimiter (':'); the left side of the word is stored as example and the right side as choice.
	d. compute the average of the difference of pairs on the left side
	e. calculate the cosine similarity using the custom defined cos_similar function against the average on left side for each pair on the right side
	f. store the least similar pairs index and the most similar pairs index in two variables
	g. the analogy is made and now the output is performed according to the format given while attaching the indices of the least and most similar
	pairs while outputting the string

Task 4. Top 20 similar word

1. calculate the cosine score for each word in the dictionary against {"first", "america","would"}
2. append each score to the corresponding list
3. sort the list in ascending order and get the last 20 elements that have higher score representing most similarity














#############################################################################################
##################     Results of Trying Different Parameters                 ###############
#############################################################################################


###############################
###############################
       Cross-Entropy
###############################
###############################


Commands on Terminal and Execution of File:
1. python word2vec_basic.py
2. python word_analogy.py      	#commented out once and uncommented loss_model = "cross_entropy"
3. ./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_cross_entropy.txt score_ce.txt
4. python top_twenty.py 	#commented out once and uncommented loss_model = "cross_entropy"


#####################
Trial 1##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   297
Number of Least Illustrative Guessed Incorrectly: 617
Accuracy of Least Illustrative Guesses:            32.5%
Number of Most Illustrative Guessed Correctly:    290
Number of Most Illustrative Guessed Incorrectly:  624
Accuracy of Most Illustrative Guesses:             31.7%
Overall Accuracy:                                  32.1%


#####################
Trial 2##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 8       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   306
Number of Least Illustrative Guessed Incorrectly: 608
Accuracy of Least Illustrative Guesses:            33.5%
Number of Most Illustrative Guessed Correctly:    296
Number of Most Illustrative Guessed Incorrectly:  618
Accuracy of Most Illustrative Guesses:             32.4%
Overall Accuracy:                                  32.9%


#####################
Trial 3##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   290
Number of Least Illustrative Guessed Incorrectly: 624
Accuracy of Least Illustrative Guesses:            31.7%
Number of Most Illustrative Guessed Correctly:    308
Number of Most Illustrative Guessed Incorrectly:  606
Accuracy of Most Illustrative Guesses:             33.7%
Overall Accuracy:                                  32.7%


#####################
Trial 4##############
#####################

  batch_size = 512
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   296
Number of Least Illustrative Guessed Incorrectly: 618
Accuracy of Least Illustrative Guesses:            32.4%
Number of Most Illustrative Guessed Correctly:    318
Number of Most Illustrative Guessed Incorrectly:  596
Accuracy of Most Illustrative Guesses:             34.8%
Overall Accuracy:                                  33.6%

#####################
Trial 5##############
#####################

  batch_size = 64
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   283
Number of Least Illustrative Guessed Incorrectly: 631
Accuracy of Least Illustrative Guesses:            31.0%
Number of Most Illustrative Guessed Correctly:    286
Number of Most Illustrative Guessed Incorrectly:  628
Accuracy of Most Illustrative Guesses:             31.3%
Overall Accuracy:                                  31.1%


#####################
Trial 6##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 0.001

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    321
Number of Most Illustrative Guessed Incorrectly:  593
Accuracy of Most Illustrative Guesses:             35.1%
Overall Accuracy:                                  34.5%


#####################
Trial 7##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 400001	# Maximum number of training steps
  learning_rate = 0.001

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   308
Number of Least Illustrative Guessed Incorrectly: 606
Accuracy of Least Illustrative Guesses:            33.7%
Number of Most Illustrative Guessed Correctly:    320
Number of Most Illustrative Guessed Incorrectly:  594
Accuracy of Most Illustrative Guesses:             35.0%
Overall Accuracy:                                  34.4%


#####################
Trial 8 ##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 8       	# How many words to consider left and right.
  num_skips = 16         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 0.00001

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    320
Number of Most Illustrative Guessed Incorrectly:  594
Accuracy of Most Illustrative Guesses:             35.0%
Overall Accuracy:                                  34.5%

#####################
Trial 9##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 2       	# How many words to consider left and right.
  num_skips = 4         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   311
Number of Least Illustrative Guessed Incorrectly: 603
Accuracy of Least Illustrative Guesses:            34.0%
Number of Most Illustrative Guessed Correctly:    307
Number of Most Illustrative Guessed Incorrectly:  607
Accuracy of Most Illustrative Guesses:             33.6%
Overall Accuracy:                                  33.8%


#####################
Trial 10##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 2       	# How many words to consider left and right.
  num_skips = 4         	# How many times to reuse an input to generate a label.
  max_num_steps  = 100001	# Maximum number of training steps
  learning_rate = 1



Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   283
Number of Least Illustrative Guessed Incorrectly: 631
Accuracy of Least Illustrative Guesses:            31.0%
Number of Most Illustrative Guessed Correctly:    299
Number of Most Illustrative Guessed Incorrectly:  615
Accuracy of Most Illustrative Guesses:             32.7%
Overall Accuracy:                                  31.8%

#####################
Trial 11##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 0.001


Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   309
Number of Least Illustrative Guessed Incorrectly: 605
Accuracy of Least Illustrative Guesses:            33.8%
Number of Most Illustrative Guessed Correctly:    320
Number of Most Illustrative Guessed Incorrectly:  594
Accuracy of Most Illustrative Guesses:             35.0%
Overall Accuracy:                                  34.4%

#####################
Trial 12##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 4.0


Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   297
Number of Least Illustrative Guessed Incorrectly: 617
Accuracy of Least Illustrative Guesses:            32.5%
Number of Most Illustrative Guessed Correctly:    291
Number of Most Illustrative Guessed Incorrectly:  623
Accuracy of Most Illustrative Guesses:             31.8%
Overall Accuracy:                                  32.2%


#####################
Trial 13##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 800001	# Maximum number of training steps
  learning_rate = 0.00001

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_cross_entropy.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    321
Number of Most Illustrative Guessed Incorrectly:  593
Accuracy of Most Illustrative Guesses:             35.1%
Overall Accuracy:                                  34.5%


###############################
###############################
            NCE
###############################
###############################


1. python word2vec_basic.py nce
2. python word_analogy.py          	#commented out cross-entropy and uncommented loss_model = "nce"
3. ./score_maxdiff.pl word_analogy_dev_mturk_answers.txt word_analogy_dev_nce.txt score_nce.txt
4. python top_twenty.py 		#commented out cross-entropy and uncommented loss_model = "nce"



#####################
Trial 1##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   294
Number of Least Illustrative Guessed Incorrectly: 620
Accuracy of Least Illustrative Guesses:            32.2%
Number of Most Illustrative Guessed Correctly:    328
Number of Most Illustrative Guessed Incorrectly:  586
Accuracy of Most Illustrative Guesses:             35.9%
Overall Accuracy:                                  34.0%


#####################
Trial 2##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   302
Number of Least Illustrative Guessed Incorrectly: 612
Accuracy of Least Illustrative Guesses:            33.0%
Number of Most Illustrative Guessed Correctly:    304
Number of Most Illustrative Guessed Incorrectly:  610
Accuracy of Most Illustrative Guesses:             33.3%
Overall Accuracy:                                  33.2%


#####################
Trial 3##############
#####################


  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 8       	# How many words to consider left and right.
  num_skips = 16         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   295
Number of Least Illustrative Guessed Incorrectly: 619
Accuracy of Least Illustrative Guesses:            32.3%
Number of Most Illustrative Guessed Correctly:    296
Number of Most Illustrative Guessed Incorrectly:  618
Accuracy of Most Illustrative Guesses:             32.4%
Overall Accuracy:                                  32.3%



#####################
Trial 4##############
#####################

  batch_size = 128
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 400001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64


Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   288
Number of Least Illustrative Guessed Incorrectly: 626
Accuracy of Least Illustrative Guesses:            31.5%
Number of Most Illustrative Guessed Correctly:    333
Number of Most Illustrative Guessed Incorrectly:  581
Accuracy of Most Illustrative Guesses:             36.4%
Overall Accuracy:                                  34.0%

#####################
Trial 5##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 4       	# How many words to consider left and right.
  num_skips = 8         	# How many times to reuse an input to generate a label.
  max_num_steps  = 100001	# Maximum number of training steps
  learning_rate = 0.01
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   298
Number of Least Illustrative Guessed Incorrectly: 616
Accuracy of Least Illustrative Guesses:            32.6%
Number of Most Illustrative Guessed Correctly:    318
Number of Most Illustrative Guessed Incorrectly:  596
Accuracy of Most Illustrative Guesses:             34.8%
Overall Accuracy:                                  33.7%


#####################
Trial 6##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 2       	# How many words to consider left and right.
  num_skips = 4         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 64

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   295
Number of Least Illustrative Guessed Incorrectly: 619
Accuracy of Least Illustrative Guesses:            32.3%
Number of Most Illustrative Guessed Correctly:    333
Number of Most Illustrative Guessed Incorrectly:  581
Accuracy of Most Illustrative Guesses:             36.4%
Overall Accuracy:                                  34.4%


#####################
Trial 7##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 1       	# How many words to consider left and right.
  num_skips = 2         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 32

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   295
Number of Least Illustrative Guessed Incorrectly: 619
Accuracy of Least Illustrative Guesses:            32.3%
Number of Most Illustrative Guessed Correctly:    323
Number of Most Illustrative Guessed Incorrectly:  591
Accuracy of Most Illustrative Guesses:             35.3%
Overall Accuracy:                                  33.8%


#####################
Trial 8##############
#####################

  batch_size = 256
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 1       	# How many words to consider left and right.
  num_skips = 2         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 8

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   300
Number of Least Illustrative Guessed Incorrectly: 614
Accuracy of Least Illustrative Guesses:            32.8%
Number of Most Illustrative Guessed Correctly:    312
Number of Most Illustrative Guessed Incorrectly:  602
Accuracy of Most Illustrative Guesses:             34.1%
Overall Accuracy:                                  33.5%

#####################
Trial 9##############
#####################

  batch_size = 512
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 1       	# How many words to consider left and right.
  num_skips = 2         	# How many times to reuse an input to generate a label.
  max_num_steps  = 200001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 8

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   298
Number of Least Illustrative Guessed Incorrectly: 616
Accuracy of Least Illustrative Guesses:            32.6%
Number of Most Illustrative Guessed Correctly:    308
Number of Most Illustrative Guessed Incorrectly:  606
Accuracy of Most Illustrative Guesses:             33.7%
Overall Accuracy:                                  33.2%


#####################
Trial 10 ###############
#####################

  batch_size = 512
  embedding_size = 128  	# Dimension of the embedding vector.
  skip_window = 1       	# How many words to consider left and right.
  num_skips = 2         	# How many times to reuse an input to generate a label.
  max_num_steps  = 400001	# Maximum number of training steps
  learning_rate = 1.0
  num_sampled = k = 8

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_dev_nce.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   298
Number of Least Illustrative Guessed Incorrectly: 616
Accuracy of Least Illustrative Guesses:            32.6%
Number of Most Illustrative Guessed Correctly:    308
Number of Most Illustrative Guessed Incorrectly:  606
Accuracy of Most Illustrative Guesses:             33.7%
Overall Accuracy:                                  33.2%






##########################################
Libraries Used in the Project
##########################################
Keras-Applications	1.0.8	1.0.8
Keras-Preprocessing	1.1.0	1.1.0
Markdown		3.1.1	3.1.1
Werkzeug		0.16.0	0.16.0
absl-py			0.8.0	0.8.1
astor			0.8.0	0.8.0
certifi			2019.9.11	2019.9.11
chardet			3.0.4	3.0.4
cycler			0.10.0	0.10.0
gast			0.2.2	0.3.2
google-pasta		0.1.7	0.1.7
grpcio			1.24.1	1.24.1
h5py			2.10.0	2.10.0
idna			2.8	2.8
kiwisolver		1.1.0	1.1.0
matplotlib		3.1.1	3.1.1
numpy			1.17.2	1.17.2
opt-einsum		3.1.0	3.1.0
pip			19.2.3	19.2.3
protobuf		3.10.0	3.10.0
pyparsing		2.4.2	2.4.2
python-dateutil		2.8.0	2.8.0
requests		2.22.0	2.22.0
scipy			1.3.1	1.3.1
setuptools		41.2.0	41.4.0
six			1.12.0	1.12.0
tensorboard		1.14.0	2.0.0
tensorflow		1.14.1	2.0.0
tensorflow-estimator	1.14.0	2.0.0
termcolor		1.1.0	1.1.0
urllib3			1.25.6	1.25.6
virtualenv		16.7.5	16.7.5
wheel			0.33.6	0.33.6
wrapt			1.11.2	1.11.2